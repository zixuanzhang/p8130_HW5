---
title: "p8130 Homework 5"
author: "Eleanor Zhang"
date: "11/30/2018"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("caret", "boot", "MPV")
```


## Read Data

R dataset ‘state.x77’ from library(faraway) contains information on 50 states from 1970s collected by US Census Bureau. The goal is to predict ‘life expectancy’ using a combination of remaining variables.

Here our main outcome is life expectancy. The rest variables constitute the pool of variables that may be used for regression model.

```{r}
library(faraway)
data(state)
state <- as.tibble(state.x77) %>% 
  janitor::clean_names() # clean variable names
```

## Explore the data

#### data description
```{r}
str(state) # 50 rows, 8 variables
names(state)
```

Data description (from R document):  

*  population: population estimate as of July 1, 1975
*  income: per capita income (1974)
*  illiteracy: illiteracy (1970, percent of population)
*  life_exp: life expectancy in years (1969–71)
*  murder: murder and non-negligent manslaughter rate per 100,000 population (1976)
*  hs_grad: percent high-school graduates (1970)
*  frost: mean number of days with minimum temperature below freezing (1931–1960) in capital or large city
*  area: land area in square miles

#### Problem 1 Explore the data and summary

Number summary

```{r}
summary(state) 
anyNA(state) # NO missing value
```

Plot visualize distributin

```{r}
par(mfrow = c(2,4))
map(state, hist)
```

Observe:  

*  skewed: population size, illteracy, area (reported by median and IQR): maybe categorize them?
*  the other distribution looks evenly shaped (reported by mean and sd)

relationship between covariates

```{r}
state %>% select(life_exp, everything()) %>% pairs()
cor(state) %>% knitr::kable()
```

Observe:

*  murder and illiteracy seems to have exponential relation
*  Area may need to be categorized
*  life expectancy are negatively associated with murder rate and illiteracy repectively linearly. There is some positive linear relation between life expectancy and high school graduates percentage and frost days.
*  Some potential colinearity: hs_grad and income, hs_grad and illiteracy, 

#### Problem 2 Automatic procedure

```{r fit with all predictors}
multi.fit <- lm(life_exp ~ ., data = state)
summary(multi.fit)
```

Comment: murder and is the most significant predictor. hs_grad is significant at 0.05 level. The other predictors are not very significant when including all other variables in the model. The adjusted R-square is penalized such that it is significantly smaller than the unadjusted one. This implies we have included unnecessary predictors in the model.

1) __Method I: Backward elimination (choose alpha_to_remove > 0.2)__

Start from there, we use backward elimination to find the "best" subset:

By looking at the summary of full model regression, backward elimination starts eliminating the one with largest p value, so we __remove area__ first

```{r remove area}
step1 <- update(multi.fit, . ~ . -area)
summary(step1)
```

Then we __remove illiteracy__

```{r remove illiteracy}
step2 <- update(step1, . ~ . -illiteracy)
summary(step2)
```

Then Then we __remove income__

```{r}
step3 <- update(step2, . ~ . -income)
summary(step3)
```

At the beginning, we set alpha to remove is 0.2. There is no further reduction of variable at the stage. 

Result: backward selection model is 

__life expectancy = 71 + 0.00005population - 0.3Murder + 0.047hs_grad - 0.006frost__


2) __Method II: Forward elimination (choose alpha to enter < 0.2)__

We begin with regression with ech single predictor and obtain their summaries

```{r}
fit_pop <- lm(life_exp ~ population, data = state)
result <- tibble(model = map(state[-4], ~lm(life_exp ~ .x, data = state))) %>% 
  mutate(result = map(model, broom::tidy)) %>% 
  select(-model) %>% 
  unnest() %>% 
  filter(term == ".x") %>% 
  select(-statistic) %>% 
  mutate(term = c("population", "income", "illiteracy", "murder", "hs_grad", "frost", "area"),
         estimate = round(estimate, digits = 6),
         std.error = round(std.error, digits = 6))
result %>% arrange(p.value) # rank by p value
```

Enter variable with smallest p value: murder

```{r}
library(broom)
forward1 <- lm(life_exp ~ murder, data = state)
```

Enter variable with the smallest p value among the rest: 

```{r}
fit1 <- update(forward1, . ~ . +population)
fit2 <- update(forward1, . ~ . +income)
fit3 <- update(forward1, . ~ . +illiteracy)
fit4 <- update(forward1, . ~ . +hs_grad)
fit5 <- update(forward1, . ~ . +frost)
fit6 <- update(forward1, . ~ . +area)

result2 <- tibble(model = map(list(fit1, fit2, fit3, fit4, fit5, fit6), summary)) %>% 
  mutate(result = map(model, tidy)) %>% 
  select(-model) %>% 
  unnest(result)

result2 %>% 
  filter(!term %in% c("(Intercept)", "murder")) %>% 
  mutate(rank_p_value = rank(p.value)) %>% 
  right_join(., result2)
```

Enter variable: hs_grad

```{r}
forward2 <- lm(life_exp ~ murder + hs_grad, data = state)
tidy(forward2)
```

Enter variable with the smallest p value among the rest: 

```{r}
fit1 <- update(forward2, . ~ . +population)
fit2 <- update(forward2, . ~ . +income)
fit3 <- update(forward2, . ~ . +illiteracy)
fit4 <- update(forward2, . ~ . +frost)
fit5 <- update(forward2, . ~ . +area)

result3 <- tibble(model = map(list(fit1, fit2, fit3, fit4, fit5), summary)) %>% 
  mutate(result = map(model, tidy)) %>% 
  select(-model) %>% 
  unnest(result)

result3 %>% 
  filter(!term %in% c("(Intercept)", "murder", "hs_grad")) %>% 
  mutate(rank_p_value = rank(p.value)) %>% 
  right_join(., result3)
```

Enter: frost

```{r}
forward3 <- lm(life_exp ~ murder + hs_grad + frost, data = state)
summary(forward3)
```

Enter variable with the smallest p value among the rest: 

```{r}
fit1 <- update(forward3, . ~ . +population)
fit2 <- update(forward3, . ~ . +income)
fit3 <- update(forward3, . ~ . +illiteracy)
fit4 <- update(forward3, . ~ . +area)

result4 <- tibble(model = map(list(fit1, fit2, fit3, fit4), summary)) %>% 
  mutate(result = map(model, tidy)) %>% 
  select(-model) %>% 
  unnest(result)

result4 %>% 
  filter(!term %in% c("(Intercept)", "murder", "hs_grad", "frost")) %>% 
  mutate(rank_p_value = rank(p.value)) %>% 
  right_join(., result4)
```

Add population

```{r}
forward4 <- lm(life_exp ~ murder + hs_grad + frost + population, data = state)
summary(forward4)
```

Enter variable with the smallest p value among the rest: 

```{r}
fit1 <- update(forward4, . ~ . +income)
fit2 <- update(forward4, . ~ . +illiteracy)
fit3 <- update(forward4, . ~ . +area)

result5 <- tibble(model = map(list(fit1, fit2, fit3), summary)) %>% 
  mutate(result = map(model, tidy)) %>% 
  select(-model) %>% 
  unnest(result)

result5 %>% 
  filter(!term %in% c("(Intercept)", "murder", "hs_grad", "frost", "population")) %>% 
  mutate(rank_p_value = rank(p.value)) %>% 
  right_join(., result5)
```

There is no additional predictor with p < 0.2, so we will not enter any other predictor. 
Hence, the forward selection model:

__life_exp ~ 71 - 0.3murder + 0.047hs_grad - 0.006frost + 0.00005population__

__Method III: stepwise regression__

```{r}
mult.fit <- lm(life_exp ~ ., data = state)
step(mult.fit, direction='backward') # select by AIC 
```

We choose the one with smallest AIC, hence the model chosen from stepwise regression procedure is:  

__life_exp = 0.00005population - 0.3murder + 0.047hs_grad - 0.006frost__


Answer question:

a) All the three procedures end up with the same model: life_exp ~ population + murder + hs_grad + frost.

b) During the forward and backward elimination procedures, the variable population is close to the not rejection region in terms of p value if we choose alpha to be 0.05. However, at this stage of exploratory analysis, we want to leverage the critical alpha value to be more inclusive and less stringent in variable selection. Therefore we keep this variable in the model.

c) illteracy vs. HS graduation rate

```{r}
cor(state$illiteracy, state$hs_grad)
```

The linear correlation between illeteracy and HS graduation rate is 0.66 and they are negatively correlated. This makes sense because lower high graduation rate can be associated with higher rate of illiteracy. The subsets in the above do not contain both variable. 


### Problem 3 Criterion based procedure

We used criterion of Cp and adjusted R square to select for the best model

```{r function of best model}
library(leaps)
best <- function(model, ...) 
{
  subsets <- regsubsets(formula(model), model.frame(model), ...)
  subsets <- with(summary(subsets),
                  cbind(p = as.numeric(rownames(which)), which, rss, rsq, adjr2, cp, bic))
  
  return(subsets)
} 
```

```{r}
best_result <- round(best(multi.fit), 4) %>% as.tibble()
best_result

par(mar=c(4,4,1,1))
par(mfrow=c(1,2))


plot(2:8, best_result$cp, xlab="No of parameters", ylab="Cp Statistic")
abline(0,1) 

plot(2:8, best_result$adjr2, xlab="No of parameters", ylab="Adj R2")
```


Comment: From the criterion of Cp and Adjusted R square, 5 parameters reach to the summit of adjusted R square with Cp smaller than number of parameters. So we decide to choose the model with 5 parameter (4 predictors):  life_exp ~ population + murder + hs_grad + frost. The model we achieved here is consistent with the automatic procedure result above.

### Problem 4 choose final model and checking assumption




    

